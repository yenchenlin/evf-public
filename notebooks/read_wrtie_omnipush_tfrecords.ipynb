{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from video_prediction import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: bair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'bair'\n",
    "input_dir = '/data/vision/phillipi/gen-models/video_prediction/data/bair'\n",
    "dataset_hparams = 'use_state=True'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoDataset = datasets.get_dataset_class(dataset)\n",
    "train_dataset = VideoDataset(\n",
    "    input_dir,\n",
    "    mode='train',\n",
    "    hparams=dataset_hparams)\n",
    "train_tf_dataset = train_dataset.make_dataset(batch_size)\n",
    "train_iterator = train_tf_dataset.make_one_shot_iterator()\n",
    "train_handle = train_iterator.string_handle()\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    train_handle, train_tf_dataset.output_types, train_tf_dataset.output_shapes)\n",
    "inputs = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.keys())\n",
    "sess = tf.Session()\n",
    "x = sess.run(inputs)\n",
    "print(x['images'].shape, x['states'].shape, x['actions'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for read/write tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_list_feature(values):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=values))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _floats_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def save_tf_record(output_fname, sequences):\n",
    "    print('saving sequences to %s' % output_fname)\n",
    "    with tf.python_io.TFRecordWriter(output_fname) as writer:\n",
    "        for (images, metadata, push_name) in sequences:\n",
    "            assert len(images) == len(metadata)\n",
    "            num_frames = len(images)\n",
    "            encoded_images = [image.tostring() for image in images]\n",
    "            states = np.concatenate((metadata[:, :2], np.zeros((num_frames, 1))), axis=1).reshape(-1)\n",
    "            actions = np.repeat(metadata[:-1, 2], 4)\n",
    "            features = tf.train.Features(feature={\n",
    "                'images/encoded': _bytes_list_feature(encoded_images),\n",
    "                'actions': _floats_list_feature(actions),\n",
    "                'states': _floats_list_feature(states),\n",
    "                'sequence_length': _int64_feature(num_frames),\n",
    "                'push_name': _bytes_feature(push_name)\n",
    "            })\n",
    "            example = tf.train.Example(features=features)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: omnipush_stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "dataset = 'omnipush_1_weight_stitch'\n",
    "dataset_dir = '/data/vision/phillipi/gen-models/svg/dataset/{}/'.format(dataset)\n",
    "output_dir = '/data/vision/phillipi/gen-models/video_prediction/data/{}/'.format(dataset)\n",
    "splits = ['train', 'val']\n",
    "for split in splits:\n",
    "    split_path = os.path.join(output_dir, split)\n",
    "    if not os.path.exists(split_path):\n",
    "        os.makedirs(split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/3a3a3a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/3a4a3c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3c4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2C4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2C4a3a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a4a2c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a4a1b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a3a2B.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1c4a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/3a4a3b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2a3a2b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a2c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a4a2c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3b4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/3a3a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a1b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3c3a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a4a4a1c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a1c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a1b3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a4a2B.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2c4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a2b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2a3a2c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a3a2C.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2C2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a2b3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2B2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a2c2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1c2a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a4a3a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2b4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c3a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a2a2C.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a1c3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2B2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c4a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1b4a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1b2a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a2b2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a2a2b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1c2a2a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2c4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a3a2c3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/3a3a3a3b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1b2a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2B3a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c1a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a4a4a1b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2C2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1c2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2a2a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1c2a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2C3a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a1c4a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2C2a3a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/2b4a3a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a4a2b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1b2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/train/1a2a3a3b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2C4a3a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a4a1b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a4a2c.tfrecord\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a3a2B.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/3a4a3b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1c4a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/3a4a3c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/3a3a3a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3c4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2C4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a1b3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a4a2B.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a1c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a4a4a1c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2c4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2a3a2c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a2b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a2c4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2a3a2b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a4a2c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3b4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/3a3a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3c3a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a1b4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2B2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c4a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1b4a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a2b2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1b2a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1c2a2a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a2a2b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/3a3a3a3b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a2c3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2c4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2C2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a3a2C.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a2b3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2B2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a2c2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1c2a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2b4a4a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a4a3a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a1c3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a3a2a2C.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c3a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2C3a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2a2a4a3c.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1c2a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c4a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2C2a3a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2b4a3a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a4a2b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2a3a3b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1b2a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/2B3a2a4a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1b2a2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a1c1a2a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a4a4a1b.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1a2C2a3a.tfrecord\n",
      "saving sequences to /data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch/val/1c2a2a4a.tfrecord\n",
      "3718\n"
     ]
    }
   ],
   "source": [
    "n_trajs = 0\n",
    "splits = ['train', 'test']\n",
    "for split in splits:\n",
    "    shape_names = os.listdir(os.path.join(dataset_dir, '{}/'.format(split)))\n",
    "    for shape_name in shape_names:\n",
    "        sequences = []\n",
    "        dnames = glob.glob(os.path.join(dataset_dir, '{}/{}/**/'.format(split, shape_name)))\n",
    "        for dname in dnames:\n",
    "            n_images = len(glob.glob(os.path.join(dname, '*.png')))\n",
    "            images = []\n",
    "            for i in range(n_images):\n",
    "                fname = os.path.join(dname, '{}.png'.format(i))\n",
    "                images.append(imread(fname))\n",
    "            metadata = np.load(os.path.join(dname, 'actions.npy'))\n",
    "            push_name = str.encode(dname.split('/')[-2])\n",
    "            sequences.append((images, metadata, push_name))\n",
    "            n_trajs += 1\n",
    "        output_split = 'val' if split == 'test' else 'train'\n",
    "        save_tf_record('/data/vision/phillipi/gen-models/video_prediction/data/{}/{}/{}.tfrecord'.format(dataset, output_split, shape_name), sequences)\n",
    "        \n",
    "print(n_trajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/vision/phillipi/gen-models/video_prediction/data/omnipush_1_weight_stitch\n"
     ]
    }
   ],
   "source": [
    "dataset_class = 'omnipush'\n",
    "input_dir = '/data/vision/phillipi/gen-models/video_prediction/data/{}'.format(dataset)\n",
    "print(input_dir)\n",
    "dataset_hparams = 'use_state=True'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoDataset = datasets.get_dataset_class(dataset_class)\n",
    "train_dataset = VideoDataset(\n",
    "    input_dir,\n",
    "    mode='train',\n",
    "    hparams=dataset_hparams)\n",
    "\n",
    "train_tf_dataset = train_dataset.make_dataset(batch_size)\n",
    "train_iterator = train_tf_dataset.make_one_shot_iterator()\n",
    "train_handle = train_iterator.string_handle()\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    train_handle, train_tf_dataset.output_types, train_tf_dataset.output_shapes)\n",
    "inputs = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['images', 'states', 'push_name', 'actions'])\n",
      "(16, 24, 64, 64, 3) (16, 24, 3) (16, 23, 4) [[b'motion_surface=abs_shape=1c4a2a3a_v=50_rep=0004_push=0000_t=0.849']\n",
      " [b'motion_surface=abs_shape=1a1c4a3a_v=50_rep=0011_push=0000_t=-2.155']\n",
      " [b'motion_surface=abs_shape=1a1c4a3a_v=50_rep=0014_push=0000_t=-2.472']\n",
      " [b'motion_surface=abs_shape=2B3a2a4a_v=50_rep=0050_push=0000_t=-0.421']\n",
      " [b'motion_surface=abs_shape=1a1a1c4a_v=50_rep=0019_push=0000_t=-0.882']\n",
      " [b'motion_surface=abs_shape=1a3b4a4a_v=50_rep=0039_push=0000_t=1.004']\n",
      " [b'motion_surface=abs_shape=1a3a3a2C_v=50_rep=0044_push=0000_t=0.893']\n",
      " [b'motion_surface=abs_shape=2B3a2a4a_v=50_rep=0000_push=0000_t=-0.854']\n",
      " [b'motion_surface=abs_shape=1a3a2b3a_v=50_rep=0056_push=0000_t=-2.361']\n",
      " [b'motion_surface=abs_shape=1a1c2a3a_v=50_rep=0030_push=0000_t=-2.440']\n",
      " [b'motion_surface=abs_shape=2C2a2a4a_v=50_rep=0041_push=0000_t=1.391']\n",
      " [b'motion_surface=abs_shape=1a3b4a4a_v=50_rep=0053_push=0000_t=0.362']\n",
      " [b'motion_surface=abs_shape=1c2a2a4a_v=50_rep=0029_push=0000_t=2.456']\n",
      " [b'motion_surface=abs_shape=1a4a3a3c_v=50_rep=0004_push=0000_t=-0.976']\n",
      " [b'motion_surface=abs_shape=1a1c4a3a_v=50_rep=0021_push=0000_t=-0.205']\n",
      " [b'motion_surface=abs_shape=1c2a2a4a_v=50_rep=0056_push=0000_t=2.152']]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "\n",
    "sess = tf.Session()\n",
    "x = sess.run(inputs)\n",
    "print(x['images'].shape, x['states'].shape, x['actions'].shape, x['push_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data from tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "def inspect_seq(seq):\n",
    "    clear_output(wait=True)\n",
    "    if os.path.exists('./tmp.gif'):\n",
    "        os.remove('./tmp.gif')\n",
    "    imageio.mimsave('./tmp.gif', seq)\n",
    "    with open('./tmp.gif','rb') as f:\n",
    "        display(Image(data=f.read(), format='gif', width=200, height=200))\n",
    "\n",
    "idx = 2\n",
    "inspect_seq(x['images'][idx])\n",
    "print(x['actions'][idx, :, 0] / np.pi * 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tfrecord with different actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_action = True\n",
    "n_trajs = 0\n",
    "splits = ['test']\n",
    "for split in splits:\n",
    "    shape_names = os.listdir(os.path.join(dataset_dir, '{}/'.format(split)))\n",
    "    for shape_name in shape_names:\n",
    "        sequences = []\n",
    "        dnames = glob.glob(os.path.join(dataset_dir, '{}/{}/**/'.format(split, shape_name)))\n",
    "        for dname in dnames:\n",
    "            n_images = len(glob.glob(os.path.join(dname, '*.png')))\n",
    "            images = []\n",
    "            for i in range(n_images):\n",
    "                fname = os.path.join(dname, '{}.png'.format(i))\n",
    "                images.append(imread(fname))\n",
    "            metadata = np.load(os.path.join(dname, 'actions.npy'))\n",
    "            # Modify the action here\n",
    "            if modify_action:\n",
    "                # Note: change all metadata to zeros!!!\n",
    "                metadata = np.zeros(metadata.shape)\n",
    "                len_push = metadata.shape[0]\n",
    "                metadata[:len_push//2, 2] = np.linspace(-0.5*np.pi, 0.5 * np.pi, num=len_push//2)\n",
    "                metadata[len_push//2:, 2] = np.linspace(0.5*np.pi, -0.5 * np.pi, num=len_push//2)\n",
    "            sequences.append((images, metadata))\n",
    "            n_trajs += 1\n",
    "        output_split = 'val_actions=S' if split == 'test' else 'train'\n",
    "        path = '/data/vision/phillipi/gen-models/video_prediction/data/{}/{}'.format(dataset, output_split)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        save_tf_record('/data/vision/phillipi/gen-models/video_prediction/data/{}/{}/{}.tfrecord'.format(dataset, output_split, shape_name), sequences)\n",
    "        \n",
    "print(n_trajs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
